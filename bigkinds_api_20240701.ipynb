{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().strftime(\"%Y%m%d\")\n",
    "base_url = \"https://news.naver.com/main/list.naver?mode=LSD&mid=shm&sid1=101&date=\" + today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_links(page):\n",
    "    url = base_url + \"&page=\" + str(page)\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.text, \"html.parser\")\n",
    "    \n",
    "    links = []\n",
    "    for a in soup.select(\"ul.type06_headline li dl dt a\"):\n",
    "        links.append(a[\"href\"])\n",
    "    for a in soup.select(\"ul.type06 li dl dt a\"):\n",
    "        links.append(a[\"href\"])\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_news_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.text, \"html.parser\")\n",
    "    \n",
    "    title_tag = soup.select_one(\"h2.media_end_head_headline\")\n",
    "    content_tag = soup.find('article',{'id':'dic_area'})\n",
    "    \n",
    "    if title_tag and content_tag:\n",
    "        title = title_tag.get_text().strip()\n",
    "        content = content_tag.get_text().strip()\n",
    "        return title, content\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 시작\n",
    "news_links = []\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    links = get_news_links(page)\n",
    "    if not links or any(link in news_links for link in links):\n",
    "        break\n",
    "    news_links.extend(links)\n",
    "    page += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 기사 내용 크롤링\n",
    "news_contents = []\n",
    "for link in news_links[:20]:    # 숫자 변경\n",
    "    try:\n",
    "        title, content = get_news_content(link)\n",
    "        news_contents.append((title, content))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get content from {link}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer # pip install sentence_transformers\n",
    "from bareunpy import Tagger # pip install bareunpy\n",
    "\n",
    "tagger = Tagger('koba-Q2CYNCI-XZ7E7PI-X6YRKPY-K4Z2KMY')\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_ext(text):\n",
    "\n",
    "    tokenized_doc = tagger.pos(text)\n",
    "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'NNG' or word[1] == 'NNP'])\n",
    "\n",
    "    n_gram_range = (1,1)\n",
    "\n",
    "    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    doc_embedding = model.encode([text])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    return mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)\n",
    "\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    # print(keywords_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title  \\\n",
      "0           \"직장 다니는 배우자 월급으로 적자 메워\"... 자영업 연체 '역대 최대'   \n",
      "1                       우리금융, 서울역 '쪽방촌' 주민들 치과 치료 돕는다   \n",
      "2               국내 조선사 잇따라 선박 수주…한화오션 2.1조·삼성중공업 1.4조   \n",
      "3   ‘6.7→8.2조’ 석달만에 23% 커진 三電 2Q 예상 영업익…반도체 반등 타고 ...   \n",
      "4              \"입맛 뚝 떨어져\" 즉석밥 뜯자 '곰팡이 득실'…제조사 입장 들어보니   \n",
      "5      “세금이 22%인데 美주식 하는 사람은 멍청이”…어떻게 생각하십니까? [투자360]   \n",
      "6                     전영현 삼성전자 부회장, 노조 만났다…\"노사 대화 노력\"   \n",
      "7       이형주 VM 컨설팅 대표, 한국 복합문화공간의 유니크 베뉴 경쟁력 연구 논문 발표   \n",
      "8                개인정보위 “알리·테무 조사 결과 이달 발표…하반기도 AI 집중”   \n",
      "9                        하이로닉, 치과임플란트 신제품 베트남 유통계약 체결   \n",
      "10      명품감정원이 “진짜” 랬는데…“가짜”라 뒤집은 중고거래플랫폼, 이런 황당한 일이?   \n",
      "\n",
      "                           Keywords  \n",
      "0             [금리, 대출, 자금, 소비자, 비용]  \n",
      "1   [우리금융그룹, 번째, 우리금융미래재단, 서울대, 이날]  \n",
      "2        [vlcc, 천연가스, 셔틀탱커, 연료, 계약]  \n",
      "3         [반도체, 삼성전자, 가격, 투자업계, 속도]  \n",
      "4             [제품, 곰팡이, 무균, 유통, 상품]  \n",
      "5         [투자자, 주식, 미국, 한국투자증권, 투자]  \n",
      "6      [노조, 전국삼성전자노동조합, 임금, 협상, 노동]  \n",
      "7     [컨설팅, vm, 서울과학종합대학원, 산업, 경영자]  \n",
      "8          [기자단, 위원회, 한국, 카카오, 위원장]  \n",
      "9         [유통업체, 베트남, 자회사, 유통망, 유통]  \n",
      "10      [브랜드, 한국명품감정원, 캡처, 판매자, 업계]  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store titles and nouns\n",
    "df = pd.DataFrame(columns=[\"Title\", \"Keywords\"])\n",
    "\n",
    "# Text analysis using Mecab and store in DataFrame\n",
    "for i, (title, content) in enumerate(set(news_contents), start=1):\n",
    "    if content:\n",
    "        keywords = keyword_ext(content)\n",
    "        new_row = pd.DataFrame({\"Title\": [title], \"Keywords\": [keywords]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)        \n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
