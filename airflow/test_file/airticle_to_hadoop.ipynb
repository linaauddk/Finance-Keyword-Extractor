{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# nest_asyncio 적용\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def fetch(session, url):\n",
    "    try:\n",
    "        async with session.get(url, timeout=aiohttp.ClientTimeout(total=3000)) as response:\n",
    "            return await response.text()\n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"Timeout error for URL: {url}\")\n",
    "        return None\n",
    "\n",
    "async def get_news_links(session, base_url, page):\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    response_text = await fetch(session, url)\n",
    "    if response_text is None:\n",
    "        return []\n",
    "    soup = BeautifulSoup(response_text, 'html.parser')\n",
    "    \n",
    "    links = []\n",
    "    for a in soup.select(\"ul.type06_headline li dl dt a\"):\n",
    "        links.append(a[\"href\"])\n",
    "    for a in soup.select(\"ul.type06 li dl dt a\"):\n",
    "        links.append(a[\"href\"])\n",
    "    \n",
    "    return links\n",
    "\n",
    "async def get_news_content(session, url):\n",
    "    response_text = await fetch(session, url)\n",
    "    if response_text is None:\n",
    "        return None, None\n",
    "    soup = BeautifulSoup(response_text, 'html.parser')\n",
    "    \n",
    "    title_tag = soup.select_one(\"h2.media_end_head_headline\")\n",
    "    content_tag = soup.find('article', {'id': 'dic_area'})\n",
    "    \n",
    "    if title_tag and content_tag:\n",
    "        title = title_tag.get_text().strip()\n",
    "        content = content_tag.get_text().strip()\n",
    "        return title, content\n",
    "    return None, None\n",
    "\n",
    "async def main():\n",
    "    today = datetime.today().strftime(\"%Y%m%d\")\n",
    "    base_url = \"https://news.naver.com/main/list.naver?mode=LSD&mid=shm&sid1=101&date=\" + today\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        news_links = set()\n",
    "        page = 1\n",
    "        \n",
    "        while True:\n",
    "            links = await get_news_links(session, base_url, page)\n",
    "            if not links or page == 500:\n",
    "                break            \n",
    "            news_links.update(links)\n",
    "            page += 1\n",
    "        \n",
    "\n",
    "        tasks = [get_news_content(session, link) for link in news_links]\n",
    "        \n",
    "        # news_contents = await asyncio.gather(*tasks)\n",
    "        news_contents = await tqdm_asyncio.gather(*tasks, desc=\"Fetching news content\")\n",
    "        # None 값을 제거\n",
    "        news_contents = [content for content in news_contents if content[0] is not None]\n",
    "\n",
    "        # 명사 추출\n",
    "        nouns_data = []\n",
    "        for title, content in news_contents:\n",
    "            nouns_data.append({'Title': title, 'content': content})\n",
    "        \n",
    "        df = pd.DataFrame(nouns_data)\n",
    "        df.to_csv(f\"{today}.csv\", index=False, encoding='utf-8-sig')\n",
    "        # df.to_csv(\"20240701.csv\",index=False,encoding='utf-8-sig')\n",
    "# Jupyter Notebook 환경에서 실행\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하둡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to HDFS at /test/2024-07-01.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pyarrow import fs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pyarrow.csv as pc\n",
    "# Get the current date and format it as a string\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define the file path in HDFS\n",
    "\n",
    "#hdfs dfs -mkdir test\n",
    "file_path = f\"/test{today}.csv\"\n",
    "classpath = subprocess.Popen([\"/home/ksk/hadoop/bin/hdfs\", \"classpath\", \"--glob\"], stdout=subprocess.PIPE).communicate()[0]\n",
    "os.environ[\"CLASSPATH\"] = classpath.decode(\"utf-8\")\n",
    "hdfs = fs.HadoopFileSystem(host='192.168.0.206', port=8020, user='ksk')\n",
    "\n",
    "# Pandas DataFrame을 PyArrow의 Table 객체로 변환\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Write the table to HDFS as a Parquet file\n",
    "with hdfs.open_output_stream(file_path) as stream:\n",
    "    # pc.write_table(table, stream)\n",
    "    pc.write_csv(table,stream)\n",
    "print(f\"DataFrame saved to HDFS at {file_path}\")\n",
    "    \n",
    "# # PyArrow를 사용하여 Parquet 포맷으로 데이터 저장\n",
    "# pq.write_table(table, file_path, filesystem=hdfs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
