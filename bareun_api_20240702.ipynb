{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().strftime(\"%Y%m%d\")\n",
    "base_url = \"https://news.naver.com/main/list.naver?mode=LSD&mid=shm&sid1=101&date=\" + today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_links(page):\n",
    "    url = base_url + \"&page=\" + str(page)\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.text, \"html.parser\")\n",
    "    \n",
    "    links = []\n",
    "    for a in soup.select(\"ul.type06_headline li dl dt a\"):\n",
    "        links.append(a[\"href\"])\n",
    "    for a in soup.select(\"ul.type06 li dl dt a\"):\n",
    "        links.append(a[\"href\"])\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_news_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.text, \"html.parser\")\n",
    "    \n",
    "    title_tag = soup.select_one(\"h2.media_end_head_headline\")\n",
    "    content_tag = soup.find('article',{'id':'dic_area'})\n",
    "    \n",
    "    if title_tag and content_tag:\n",
    "        title = title_tag.get_text().strip()\n",
    "        content = content_tag.get_text().strip()\n",
    "        return title, content\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 시작\n",
    "news_links = []\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    links = get_news_links(page)\n",
    "    if not links or any(link in news_links for link in links):\n",
    "        break\n",
    "    news_links.extend(links)\n",
    "    page += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 기사 내용 크롤링\n",
    "news_contents = []\n",
    "for link in news_links[:20]:    # 숫자 변경\n",
    "    try:\n",
    "        title, content = get_news_content(link)\n",
    "        news_contents.append((title, content))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get content from {link}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/project/third_project/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer # pip install sentence_transformers\n",
    "from bareunpy import Tagger # pip install bareunpy\n",
    "\n",
    "tagger = Tagger('koba-Q2CYNCI-XZ7E7PI-X6YRKPY-K4Z2KMY')\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_ext(text):\n",
    "\n",
    "    tokenized_doc = tagger.pos(text)\n",
    "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'NNG' or word[1] == 'NNP'])\n",
    "\n",
    "    n_gram_range = (1,1)\n",
    "\n",
    "    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    doc_embedding = model.encode([text])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    return mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)\n",
    "\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    # print(keywords_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bareunpy import Tagger\n",
    "from datetime import datetime\n",
    "\n",
    "def keyword_ext(text):\n",
    "    \"\"\"텍스트에서 키워드를 추출합니다.\"\"\"\n",
    "    tokenized_doc = tagger.pos(text)\n",
    "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'NNG' or word[1] == 'NNP'])\n",
    "    if not tokenized_nouns.strip():\n",
    "        return []\n",
    "    \n",
    "    count = CountVectorizer(ngram_range=(1, 1)).fit([tokenized_nouns])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    if len(candidates) == 0:\n",
    "        return []\n",
    "\n",
    "    doc_embedding = model.encode([text])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    return mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)\n",
    "\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "    \"\"\"다양성을 고려한 최대 마진 적중률(MMR)을 계산하여 키워드를 추출합니다.\"\"\"\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    if len(word_doc_similarity) == 0 or len(word_similarity) == 0:\n",
    "        return []\n",
    "\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "        mmr = (1 - diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "\n",
    "        if mmr.size == 0:\n",
    "            break\n",
    "\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]\n",
    "\n",
    "# 뉴스 내용을 포함한 CSV 파일 읽기\n",
    "today = datetime.today().strftime(\"%Y%m%d\")\n",
    "news_contents = pd.read_csv(f\"{today}.csv\", encoding='utf-8')\n",
    "news_contents = news_contents.dropna()\n",
    "\n",
    "# 제목과 키워드를 저장할 DataFrame 생성\n",
    "df = pd.DataFrame(columns=[\"Title\", \"Keywords\", \"Keyword_Count\"])\n",
    "\n",
    "# 키워드 추출 및 DataFrame에 저장\n",
    "for i, row in news_contents.iterrows():\n",
    "    title = row['Title']\n",
    "    content = row['content']\n",
    "    if content:\n",
    "        keywords = keyword_ext(content)\n",
    "        new_row = pd.DataFrame({\"Title\": [title], \"Keywords\": [keywords], \"Keyword_Count\": [len(keywords)]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# DataFrame 출력\n",
    "print(df)\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "df.to_csv(f\"{today}_keywords1.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 키워드와 그 빈도를 저장할 DataFrame 생성\n",
    "all_keywords = []\n",
    "\n",
    "for keywords in df['Keywords']:\n",
    "    all_keywords.extend(keywords)\n",
    "\n",
    "keyword_counts = pd.Series(all_keywords).value_counts().reset_index()\n",
    "keyword_counts.columns = ['Keywords', 'Count']\n",
    "\n",
    "# 최종 키워드 빈도수 DataFrame 출력\n",
    "print(keyword_counts)\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "keyword_counts.to_csv(f\"{today}_keyword_counts1.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Title  \\\n",
      "0                         韓 기업 발목 잡는 상속세… “자본이득세로 전환해야”   \n",
      "1                          SK온 \"흑자전환까지 임원연봉 동결\" 비상경영 돌입   \n",
      "2                  `영업이익률 66%` 그 회사…김형태 시프트업 대표, 1조 부호로   \n",
      "3                              [속보] 코스피, 종가 기준 2800선 탈환   \n",
      "4     정태영 부회장 “해외 진출 초석 쌓는다”…비자 CEO·글로벌 금융사 CEO들 현대카...   \n",
      "...                                                 ...   \n",
      "3951                      GE헬스케어 코리아, 김용덕 신임 대표이사 사장 선임   \n",
      "3952                       화성산업 'HS화성'으로 이름 바꾸고 새 출발 선언   \n",
      "3953                 신한·토스證, 클라우드 기반 오픈플랫폼 추진 위한 MOU 체결   \n",
      "3954                 현대차, 인도네시아서 '현지 배터리' 탑재 코나 일렉트릭 생산   \n",
      "3955        최태원, 美 출장서 아마존·인텔 CEO 잇따라 만나…\"AI 보폭 맞춰 뛰어야\"   \n",
      "\n",
      "                                     Keywords Keyword_Count  \n",
      "0           [한국조세정책학회장, 경제, 토론자, 한국경제연구원, 미래]             5  \n",
      "1                       [임원, 효율, 회의, 경영자, 조직]             5  \n",
      "2                    [인수, 자금, nh투자증권, 주식, 가치]             5  \n",
      "3                 [지난달, 포인트, 거래, 연합뉴스코스피, 지수]             5  \n",
      "4     [아프리카국립상업은행, 페이스북, 현대카드, 디스커버리은행, 알고리즘]             5  \n",
      "...                                       ...           ...  \n",
      "3951                   [매니저, 김용덕, 기업, 총괄, 사장]             5  \n",
      "3952                  [화성, 오전, 수성구, 화성산업, 제공]             5  \n",
      "3953                [토스증권, 협약, 사용자, 회사, 파트너사]             5  \n",
      "3954             [동남아시아, 배터리셀, 현대차그룹, 공장, 서울]             5  \n",
      "3955                    [회장, 인텔, 회의, 자회사, 리더]             5  \n",
      "\n",
      "[3956 rows x 3 columns]\n",
      "     Keywords  Count\n",
      "0          서울    460\n",
      "1          시장    295\n",
      "2        2024    174\n",
      "3          한국    169\n",
      "4          금리    162\n",
      "...       ...    ...\n",
      "4169      충전소      1\n",
      "4170      김지민      1\n",
      "4171   디자인진흥원      1\n",
      "4172       국어      1\n",
      "4173      수성구      1\n",
      "\n",
      "[4174 rows x 2 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
